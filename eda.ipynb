{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdcast as pdc   # Firstly you should run 'pip install pandas-downcast'\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_optimize(df) :\n",
    "        \"\"\"Function to down cast pandas entries types into smaller versions in order to optimize space in the used dataframes\n",
    "    returns the resulting dataframe\n",
    "    \"\"\"\n",
    "        \n",
    "        df = pdc.downcast(df)\n",
    "        return df \n",
    "\n",
    "def df_optimize_path(df_path :str) :\n",
    "\n",
    "    \"\"\"Function to down cast pandas entries types into smaller versions in order to optimize space in the used dataframes\n",
    "    returns the resulting dataframe\n",
    "    \"\"\"\n",
    " \n",
    "    df = pd.read_csv(df_path)\n",
    "    a =  df.memory_usage().sum()\n",
    "    df = pdc.downcast(df)\n",
    "    b = df.memory_usage().sum()\n",
    "    print(\"Memory saved by : \" ,((1-b/a) * 100),\"%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def is_unique_key(df : pd.DataFrame ,columnName : str) :\n",
    "\n",
    "    \"\"\"Function to check if column is a primary key in the given dataframe    \n",
    "    returns true if the entry is primary key, false otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    x = df.shape[0]\n",
    "    l = df[columnName].drop_duplicates().shape[0]\n",
    "\n",
    "    return x == l  \n",
    "\n",
    "def df_aggreg(df ,on ,aggreg_dict) : \n",
    "     \"\"\"Function to do an aggregation on Dataframe columns on a specific function given in the aggreg_dict\"\"\"\n",
    "     df = df.groupby(on).agg(aggreg_dict).reset_index()\n",
    "     df.columns = [f'{col[0]}_{col[1].lower()}' if isinstance(col, tuple) else col.lower() for col in df.columns]\n",
    "     df.rename(columns = {\"sk_id_prev_\": \"sk_id_prev\", \"sk_id_curr_first\" : \"sk_id_curr\",\"sk_id_curr_\" : \"sk_id_curr\",\"sk_id_bureau_\":\"sk_id_bureau\"},inplace=True)\n",
    "     return df\n",
    "     \n",
    "      \n",
    "def last(x) : \n",
    "    return x.iloc[-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and DownCasting of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive - Université Mohammed VI Polytechnique\\.S1\\CSCI-LBD111 Data Visualization\\Project\\eda.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Universit%C3%A9%20Mohammed%20VI%20Polytechnique/.S1/CSCI-LBD111%20Data%20Visualization/Project/eda.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loan_applications \u001b[39m=\u001b[39m df_optimize_path(\u001b[39m\"\u001b[39;49m\u001b[39mDataset/loan_applications_train.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Universit%C3%A9%20Mohammed%20VI%20Polytechnique/.S1/CSCI-LBD111%20Data%20Visualization/Project/eda.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m previous_credits \u001b[39m=\u001b[39m df_optimize_path(\u001b[39m\"\u001b[39m\u001b[39mDataset/previous_credits.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Universit%C3%A9%20Mohammed%20VI%20Polytechnique/.S1/CSCI-LBD111%20Data%20Visualization/Project/eda.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m credit_bureau_balance\u001b[39m=\u001b[39mdf_optimize_path(\u001b[39m\"\u001b[39m\u001b[39mDataset/credit_bureau_balance.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\OneDrive - Université Mohammed VI Polytechnique\\.S1\\CSCI-LBD111 Data Visualization\\Project\\eda.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Universit%C3%A9%20Mohammed%20VI%20Polytechnique/.S1/CSCI-LBD111%20Data%20Visualization/Project/eda.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdf_optimize_path\u001b[39m(df_path :\u001b[39mstr\u001b[39m) :\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Universit%C3%A9%20Mohammed%20VI%20Polytechnique/.S1/CSCI-LBD111%20Data%20Visualization/Project/eda.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Function to down cast pandas entries types into smaller versions in order to optimize space in the used dataframes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Universit%C3%A9%20Mohammed%20VI%20Polytechnique/.S1/CSCI-LBD111%20Data%20Visualization/Project/eda.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    returns the resulting dataframe\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Universit%C3%A9%20Mohammed%20VI%20Polytechnique/.S1/CSCI-LBD111%20Data%20Visualization/Project/eda.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Universit%C3%A9%20Mohammed%20VI%20Polytechnique/.S1/CSCI-LBD111%20Data%20Visualization/Project/eda.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(df_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Universit%C3%A9%20Mohammed%20VI%20Polytechnique/.S1/CSCI-LBD111%20Data%20Visualization/Project/eda.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     a \u001b[39m=\u001b[39m  df\u001b[39m.\u001b[39mmemory_usage()\u001b[39m.\u001b[39msum()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Universit%C3%A9%20Mohammed%20VI%20Polytechnique/.S1/CSCI-LBD111%20Data%20Visualization/Project/eda.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     df \u001b[39m=\u001b[39m pdc\u001b[39m.\u001b[39mdowncast(df)\n",
      "File \u001b[1;32md:\\Adnane\\miniconda3\\envs\\LBD_CI1S1\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\Adnane\\miniconda3\\envs\\LBD_CI1S1\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32md:\\Adnane\\miniconda3\\envs\\LBD_CI1S1\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1705\u001b[0m         nrows\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Adnane\\miniconda3\\envs\\LBD_CI1S1\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32md:\\Adnane\\miniconda3\\envs\\LBD_CI1S1\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Adnane\\miniconda3\\envs\\LBD_CI1S1\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Adnane\\miniconda3\\envs\\LBD_CI1S1\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Adnane\\miniconda3\\envs\\LBD_CI1S1\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Adnane\\miniconda3\\envs\\LBD_CI1S1\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "loan_applications = df_optimize_path(\"Dataset/loan_applications_train.csv\")\n",
    "previous_credits = df_optimize_path(\"Dataset/previous_credits.csv\")\n",
    "credit_bureau_balance=df_optimize_path(\"Dataset/credit_bureau_balance.csv\")\n",
    "previous_pos_cash_loans=df_optimize_path(\"Dataset/previous_POS_cash_loans.csv\")\n",
    "previous_credit_cards=df_optimize_path(\"Dataset/previous_credit_cards.csv\")\n",
    "previous_loan_applications=df_optimize_path(\"Dataset/previous_loan_applications.csv\")\n",
    "repayment_history=df_optimize_path(\"Dataset/repayment_history.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for unique ID's (Primary Keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(is_unique_key(loan_applications,\"sk_id_curr\"))  # True\n",
    "print(is_unique_key(previous_loan_applications,\"sk_id_prev\")) # True\n",
    "print(is_unique_key(previous_credits,\"sk_id_bureau\")) # True\n",
    "print(is_unique_key(previous_credit_cards,\"sk_id_curr\")) # False\n",
    "print(is_unique_key(previous_credit_cards,\"sk_id_prev\")) # False\n",
    "print(is_unique_key(credit_bureau_balance,\"sk_id_bureau\")) # False\n",
    "print(is_unique_key(previous_pos_cash_loans,\"sk_id_prev\")) # False\n",
    "print(is_unique_key(repayment_history,\"sk_id_curr\")) #False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_applications = pd.get_dummies(loan_applications, columns=loan_applications.select_dtypes(include=['category']).columns)\n",
    "previous_credits = pd.get_dummies(previous_credits, columns=previous_credits.select_dtypes(include=['category']).columns)\n",
    "credit_bureau_balance = pd.get_dummies(credit_bureau_balance, columns=credit_bureau_balance.select_dtypes(include=['category']).columns)\n",
    "previous_pos_cash_loans = pd.get_dummies(previous_pos_cash_loans, columns=previous_pos_cash_loans.select_dtypes(include=['category']).columns)\n",
    "previous_credit_cards = pd.get_dummies(previous_credit_cards, columns=previous_credit_cards.select_dtypes(include=['category']).columns)\n",
    "previous_loan_applications = pd.get_dummies(previous_loan_applications, columns=previous_loan_applications.select_dtypes(include=['category']).columns)\n",
    "repayment_history = pd.get_dummies(repayment_history, columns=repayment_history.select_dtypes(include=['category']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Aggregation for previous_credit_cards\n",
    "aggregations_prev_credit_cards = {\n",
    "    'sk_id_curr': 'first',\n",
    "    'months_balance': 'max',\n",
    "    'amt_balance': 'max',\n",
    "    'amt_balance': 'sum',\n",
    "    'amt_credit_limit_actual': 'max',\n",
    "    'amt_payment_current': 'mean',\n",
    "    'amt_inst_min_regularity': 'mean',\n",
    "    'sk_dpd': 'max',\n",
    "    'sk_dpd_def': 'max'\n",
    "}\n",
    "aggregations_prev_pos_cash_loans = {\n",
    "    'sk_id_curr': 'first',\n",
    "    'months_balance': 'max',\n",
    "    'cnt_instalment': 'mean',\n",
    "    'cnt_instalment_future': 'sum',\n",
    "    'sk_dpd': 'max',\n",
    "    'sk_dpd_def': 'max',\n",
    "    'name_contract_status_Active': 'sum', \n",
    "    'name_contract_status_Amortized debt': 'sum',\n",
    "    'name_contract_status_Approved': 'sum',\n",
    "    'name_contract_status_Canceled': 'sum',\n",
    "    'name_contract_status_Completed': 'sum',\n",
    "    'name_contract_status_Demand': 'sum',\n",
    "    'name_contract_status_Returned to the store': 'sum',\n",
    "    'name_contract_status_Signed': 'sum',\n",
    "    'name_contract_status_XNA': 'sum'\n",
    "}\n",
    "aggregations_repayment_history = {\n",
    "    'sk_id_curr': 'first',\n",
    "    'num_instalment_version': 'nunique',\n",
    "    'num_instalment_number': 'max',\n",
    "    'days_instalment': ['min', 'max'],\n",
    "    'days_entry_payment': ['min', 'max'],\n",
    "    'amt_instalment': 'sum',\n",
    "    'amt_payment': 'sum'\n",
    "}\n",
    "aggregations_credit_bureau_balance = {\n",
    "    'months_balance': ['min', 'max'],\n",
    "    'status_0': 'last',\n",
    "    'status_1': 'last',\n",
    "    'status_2': 'last',\n",
    "    'status_3': 'last',\n",
    "    'status_4': 'last',\n",
    "    'status_5': 'last',\n",
    "    'status_C': 'last',\n",
    "    'status_X': 'last'\n",
    "}\n",
    "previous_credit_cards = df_aggreg(previous_credit_cards,\"sk_id_prev\",aggregations_prev_credit_cards)\n",
    "print(is_unique_key(previous_credit_cards,\"sk_id_prev\"))\n",
    "\n",
    "previous_pos_cash_loans = df_aggreg(previous_pos_cash_loans,\"sk_id_prev\",aggregations_prev_pos_cash_loans)\n",
    "print(is_unique_key(previous_pos_cash_loans,\"sk_id_prev\"))\n",
    "\n",
    "repayment_history = df_aggreg(repayment_history,\"sk_id_prev\",aggregations_repayment_history)\n",
    "print(is_unique_key(repayment_history,\"sk_id_prev\"))\n",
    "\n",
    "credit_bureau_balance = df_aggreg(credit_bureau_balance,'sk_id_bureau',aggregations_credit_bureau_balance)\n",
    "print(is_unique_key(credit_bureau_balance,\"sk_id_bureau\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inplace merging and downcasting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First merge done\n",
      "Second merge done\n",
      "Third merge done\n",
      "Fourth merge done\n",
      "Section Two merge Done\n",
      "___________________________\n",
      "Merging Section 2 with Section 3 ...\n",
      "Fifth merge done\n",
      "___________________________\n",
      "Merging Section 1 with the other Sections ...\n",
      "Merge Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "section2 = pd.merge( previous_loan_applications,previous_credit_cards ,on = [\"sk_id_prev\",\"sk_id_curr\"],how = \"left\")\n",
    "\n",
    "print(\"First merge done\")\n",
    "\n",
    "section2 = pd.merge(section2,previous_pos_cash_loans ,on=[\"sk_id_prev\",\"sk_id_curr\"],how=\"inner\")\n",
    "\n",
    "print(\"Second merge done\")\n",
    "\n",
    "\n",
    "section2 = pd.merge(section2,repayment_history,on=[\"sk_id_prev\",\"sk_id_curr\"],how='inner')\n",
    "\n",
    "print(\"Third merge done\")\n",
    "\n",
    "print('Section Two merge Done')\n",
    "print(\"___________________________\")\n",
    "print(\"Merging inside Section 3 ...\")\n",
    "\n",
    "section3 = pd.merge(previous_credits,credit_bureau_balance,on=\"sk_id_bureau\",how=\"inner\")\n",
    "\n",
    "print(\"Fourth merge done\")\n",
    "\n",
    "section3_2 = pd.merge(section2, section3, on=\"sk_id_curr\", how=\"inner\")\n",
    "\n",
    "section1 = loan_applications\n",
    "\n",
    "full = pd.merge(section1, section3_2, on=\"sk_id_curr\", how=\"inner\")\n",
    "\n",
    "print(\"Merge Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
